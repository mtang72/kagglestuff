{
  "cells": [
    {
      "metadata": {
        "_uuid": "0451499c94808b73452aa054397a4de5cb8dbcbb"
      },
      "cell_type": "markdown",
      "source": "### This kernel is my submission to the House Prices: Advanced Regression Technique tutorial dataset. It's my first time making a IPython notebook or working with Kaggle datasets so it's mostly for learning's sake and will follow the practices of others, but I'm excited to see what results it'll get.\n\nThe steps I will walk through are:\n1. Data Summarization\n2. Data Cleaning\n3. Data Exploration\n4. Feature Engineering\n5. Trying different algorithms (current planned: linear & logistic regression, SVM, random forests)\n6. Comparing accuracy rates"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "67d02777a10e6a5679fbd28a47801743e709304e"
      },
      "cell_type": "code",
      "source": "#package imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "998359febe3c62c3d4515841c088fdec67581f9c"
      },
      "cell_type": "markdown",
      "source": "# 1. Data Summarization"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "43ce272462c6038dc41ce9ebd9825d8bc3926560"
      },
      "cell_type": "code",
      "source": "train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "190ec7374dcfc2094b9e2e0d43376251aff81ff3"
      },
      "cell_type": "code",
      "source": "train.info()\ntrain[train.PoolArea!=0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "efa25d95ded2c315fc3a972b6a54b84b66503255"
      },
      "cell_type": "code",
      "source": "test.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "199898a7bb94ff21ad8a13774c0ab2e4589d5c45"
      },
      "cell_type": "markdown",
      "source": "### Observations:\n- One response variable, SalePrice\n- Only 7 houses have pools, with PoolArea != 0 and PoolQC not NaN\n- Only ~271 houses have fences, 91 have alleys, 54 \"MiscFeature\", 770 fireplaces\n- Lower amts of null vals in LotFrontage, MasVnrType, MasVnrArea, BsmtQual, BsmtCont, BsmtExposure, BsmtFinType1, BsmtFinSF1, BsmtFinType2, Electrical, FireplaceQu, garage quals (GarageCars and GarageArea are 0 when other garage quals are null)\n- Test cols that are missing data: LotFrontage, Alley, Utilities, Exterior1st and 2nd, MasVnrType and Area, Bsmt qualities, Garage qualities, Functional, FireplaceQu, Pool quals, Fence, Misc quals, SaleType"
    },
    {
      "metadata": {
        "_uuid": "b0ac4302606254909e2a3405344c44ab2976eda2"
      },
      "cell_type": "markdown",
      "source": "# 2. Data Cleaning"
    },
    {
      "metadata": {
        "_uuid": "0031cfb26c0e13d44a7dde2ce6c6d1cd911feaf2"
      },
      "cell_type": "markdown",
      "source": "Certain features being missing would be significant to a buyer's judgement. Therefore, we will fill in those in with 0s for NaN values to represent absence. Even the categorical values are filled with 0s, seeing as the other categories will be changed to ints for regression anyways."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4bbd10662fc3341b2e9a5b76a0a11ad618f05c6"
      },
      "cell_type": "code",
      "source": "train.fillna(0,inplace=True)\ntest.fillna(0,inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7c9e904f6c528115d581705ec205c180439d6175"
      },
      "cell_type": "markdown",
      "source": "Check to ensure no null values remain."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "a2f2cd68919113da487f0cd4674e6d776dd1b404"
      },
      "cell_type": "code",
      "source": "print(\"Train Null Values\")\nprint(train.isnull().sum().sum())\nprint(\"Test Null Values\")\nprint(test.isnull().sum().sum())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e874693685e5f70826e59ceb140d5b559dfbef4d"
      },
      "cell_type": "markdown",
      "source": "So prior to this, I tried manually analyzing and picking out features to remove. That is a pain. Let's experiment instead with four sklearn methods of feature streamlining: SelectKBest, RFE, PCA, and Extra Trees."
    },
    {
      "metadata": {
        "_uuid": "77db84916a6686064861f39e7cd160be381a36c5"
      },
      "cell_type": "markdown",
      "source": "### Desired Number of Features"
    },
    {
      "metadata": {
        "_uuid": "a0830571553fba233e17ac3301cfe79008b410f2"
      },
      "cell_type": "markdown",
      "source": "Let's take a look at the features of our data:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3fb19c6bea6c55cd07223aa9afa80a91443ec67"
      },
      "cell_type": "code",
      "source": "train.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "368cfe0608eeca7fdf4ec1c00454edbe363fbf1e"
      },
      "cell_type": "markdown",
      "source": "ID is not a feature, so removing that we have 80 features. Skimming over and looking at the number of keyword redundancies we can first estimate that we could do with just 1/4rd (20) of the present features. This figure can be adjusted later."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef5ed7568e97a392a03d71d301f6176a77dec63b"
      },
      "cell_type": "code",
      "source": "train.drop(columns='Id',inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "14bd5d16c3fe61bbe12b33047aace24e3152ef1b"
      },
      "cell_type": "markdown",
      "source": "### First Run"
    },
    {
      "metadata": {
        "_uuid": "5ee9cccafd3e5287085ee90f04ad3c3fdca32714"
      },
      "cell_type": "markdown",
      "source": "We're first going to run the random forests model to see how bad overfitting is currently, and to get the importance of each feature in order to take the top 20."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "455706c284c5f7f7b5315f9417f6bec5c25828ca"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\ntrain = pd.get_dummies(train)\nX = train.drop(columns='SalePrice')\ny = train.SalePrice\nX_train,X_val,y_train,y_val = train_test_split(X,y,random_state=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e8464f76c77884a6b4e394c03e07c4fd4692c58b"
      },
      "cell_type": "markdown",
      "source": "Importing packages:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d98ef4ceff6c6a17c9062eed55d1ea1a0ecc105"
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, mean_absolute_error",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "03962cbe6120b3a4dd28695f1eca272eb7810b55"
      },
      "cell_type": "markdown",
      "source": "Setting up, fitting and measuring accuracy of model:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24b76c282c3209295a21a076d63aec80a675efed"
      },
      "cell_type": "code",
      "source": "def rfwithval(xt, yt, xv, yv,maxfeat=\"auto\",maxnd=None,maxdp=None,ntrees=10):\n    forest = RandomForestClassifier(random_state=1,max_features=maxfeat,max_leaf_nodes=maxnd,max_depth=maxdp,n_estimators=ntrees)\n    forest.fit(xt, yt)\n    train_predictions = forest.predict(xt)\n    val_predictions = forest.predict(xv)\n    print(\"Training MAE: {} | Validation MAE: {}\"\\\n          .format(mean_absolute_error(yt,train_predictions),mean_absolute_error(yv,val_predictions)))\n    return forest\n    \nforest = rfwithval(X_train,y_train,X_val,y_val)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4f7d61fb00e9e6040e1405b4ce651fbdbb6d27d8"
      },
      "cell_type": "markdown",
      "source": "So there's room for improvement. Let's see if we can fix this by fiddling with maximum features sampled, maximum leaf nodes, maximum tree depth, and number of trees."
    },
    {
      "metadata": {
        "_uuid": "8326eb9a28223d1e185d45606663b86e4e251111"
      },
      "cell_type": "markdown",
      "source": "# 3. Tuning the Random Forest"
    },
    {
      "metadata": {
        "_uuid": "18b66407aec8a38c509378ce332c63ae062a2394"
      },
      "cell_type": "markdown",
      "source": "### Features Sampled"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "21763e0ff88b65098ef8b2da26b4707304485674"
      },
      "cell_type": "code",
      "source": "feats = [4,8,16,32,64]\nfor item in feats:\n    print(\"{} features sampled\".format(item))\n    forest1 = rfwithval(X_train,y_train,X_val,y_val,maxfeat=item)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5b3bfe24fda882cc2223784cfea44aeff1fa4a33"
      },
      "cell_type": "markdown",
      "source": "Not much of a difference is made."
    },
    {
      "metadata": {
        "_uuid": "f6995b10c3afb599fa7d593193bd1497f9719f29"
      },
      "cell_type": "markdown",
      "source": "### Leaf Nodes"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "acc06d55eb667c401201d3dd97f484ef16e03b11"
      },
      "cell_type": "code",
      "source": "nds = [3,30,300,3000,30000]\nfor item in nds:\n    print(\"{} nodes\".format(item))\n    forest1 = rfwithval(X_train,y_train,X_val,y_val,maxnd=item)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1733fba259390e42cbc301c963fec29f95ecb34b"
      },
      "cell_type": "markdown",
      "source": "Maximum 300 leaf nodes minimizes validation MAE substantially but also is far from the minimum training MAE."
    },
    {
      "metadata": {
        "_uuid": "c84971fe048e8fcd03c09806eda68b59a288f6af"
      },
      "cell_type": "markdown",
      "source": "### Depth"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cb902e20e4b7a4fc035e9dbeb2f909971496604e"
      },
      "cell_type": "code",
      "source": "dps = [3,9,27,81,243]\nfor item in dps:\n    print(\"{} depth\".format(item))\n    forest1 = rfwithval(X_train,y_train,X_val,y_val,maxdp=item)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2f9c06e56793c6f9c8c27c5813c386313edd6ac3"
      },
      "cell_type": "markdown",
      "source": "Maximum depth of 27 minimizes validation MAE substantially and does not have minimum training MAE but has not as bad of an increase."
    },
    {
      "metadata": {
        "_uuid": "51cbddc1786de436351962d8b2ac73d495ab9b4c"
      },
      "cell_type": "markdown",
      "source": "### Number of Trees"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "27aff562dadb7eecc71545fee300cadccf867562"
      },
      "cell_type": "code",
      "source": "nums = [5,10,20,40,80]\nfor item in nums:\n    print(\"{} trees\".format(item))\n    forest1 = rfwithval(X_train,y_train,X_val,y_val,ntrees=item)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bdf53a65fb70b7246c5ca96a185ce55c87d81c58"
      },
      "cell_type": "markdown",
      "source": "Increasing the number of trees eliminates training MAE but doesn't do much for validation MAE, and also increases training time substantially."
    },
    {
      "metadata": {
        "_uuid": "eecfe3e5b7c2f1921258c0c78b31e1d5023b11c0"
      },
      "cell_type": "markdown",
      "source": "## Another Idea: Using the Feature Rankings from RandomForestClassifier"
    },
    {
      "metadata": {
        "_uuid": "bbea2c46f40c6bd222183cb0f763d51a0ab6afd1"
      },
      "cell_type": "markdown",
      "source": "Another way to reduce overfitting is by reducing the number of features input into the model. When a random forest classifier is fit, it also assigns an importance score to each feature. Let's see if we can reduce overfitting by only taking n highest importance features."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1326e85f7e39f3fbfd502359d42242e28a8c2f0"
      },
      "cell_type": "code",
      "source": "#Get scores of features\nimps = forest.feature_importances_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9f9e7134cb778a67c762e7df267d25cc5378ea1"
      },
      "cell_type": "code",
      "source": "def featurereducedforest(df, imps, cut):\n    #Get cutoff for top n scores\n    cutoff = min(sorted(imps)[-cut:])\n    #Get indexes of top n\n    inds = [i for i in range(len(imps)) if imps[i]>=cutoff]\n    newdict = {i:df.iloc[:,i] for i in inds}\n    train1 = pd.DataFrame(newdict, columns=inds)\n    X_train,X_val,y_train,y_val = train_test_split(df,y,random_state=0)\n    return rfwithval(X_train,y_train,X_val,y_val)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c846e99ac87993b5e4aedbb5eba5bc2d444a2ad"
      },
      "cell_type": "code",
      "source": "feats = [5,10,20,40,80]\nfor item in feats:\n    print(\"{} features\".format(item))\n    forest1 = featurereducedforest(X,imps,item)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "73f47ce3db7bf164df43fadf51c8ee21d827c43e"
      },
      "cell_type": "markdown",
      "source": "Not much of a difference."
    },
    {
      "metadata": {
        "_uuid": "1fc4b725aa876c0450482c8359aaa537f32d0af4"
      },
      "cell_type": "markdown",
      "source": "# 4. Running With the Best Tuning"
    },
    {
      "metadata": {
        "_uuid": "802985076816f7d53604cbdfa30cf1a2ab1b5d2c"
      },
      "cell_type": "markdown",
      "source": "Let's try combining the overfitting reduction measures above."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e04e183f6a3e932e4de854162f930160b75ff494"
      },
      "cell_type": "code",
      "source": "forest1 = rfwithval(X_train,y_train,X_val,y_val,maxnd=300,maxdp=27)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c5d7e5190125196ede875536eb2db594333af5c"
      },
      "cell_type": "code",
      "source": "forest1 = rfwithval(X_train,y_train,X_val,y_val,maxdp=27,ntrees=80)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2233a7a7e2ff0517eb32db2f305f413f6034a159"
      },
      "cell_type": "code",
      "source": "forest1 = rfwithval(X_train,y_train,X_val,y_val,maxnd=300,ntrees=80)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b8e88f64cc779b05356842d11a4217f67e7163ad"
      },
      "cell_type": "markdown",
      "source": "None of the reduction measures work well in combination. So we'll go with capping the max depth at 27."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1dd4addc6c0bdcd5ea86e4c616fc84c309465c93"
      },
      "cell_type": "code",
      "source": "forest = RandomForestClassifier(random_state=1,max_depth=27)\nX_test = pd.get_dummies(test.drop(columns='Id'))\nX, X_test = X.align(X_test,join='outer',axis=1,fill_value=0)\nforest.fit(X,y)\ntest_preds = forest.predict(X_test)\noutput = pd.DataFrame({'Id':test.Id, 'SalePrice':test_preds})\noutput.to_csv('submission.csv',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e2754676f5c6cff13576da4a74709b2447b03320"
      },
      "cell_type": "markdown",
      "source": "# 5. What in the fuck"
    },
    {
      "metadata": {
        "_uuid": "bc384562a7f9513410889ae147fdf504c3109920"
      },
      "cell_type": "markdown",
      "source": "Somehow, I performed worse on this than in the tutorial model. Some more feature engineering will have to be done after all."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b404415423feb78d1662dad4f608208f143b2832"
      },
      "cell_type": "markdown",
      "source": "# 6. Feature Engineering"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aba465e8445a385f3724c192b552dfc6ac6e8e89"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}